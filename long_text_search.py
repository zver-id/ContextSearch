import re
from typing import List, Dict, Any, Tuple, Optional
import numpy as np
#from onnxruntime.transformers.models.stable_diffusion.demo_utils import max_batch
from sentence_transformers import SentenceTransformer
from razdel import sentenize
import chromadb
from chromadb.config import Settings
from tqdm import tqdm
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LongTextSearchEngine:
    def __init__(self, model_name: str = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',
                 chroma_db_path: str = "./chroma_db"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞
        :param model_name: –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ Sentence Transformers
        :param chroma_db_path: –ø—É—Ç—å –¥–æ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        """
        self.model = SentenceTransformer(model_name)
        self.chroma_db_path = chroma_db_path
        self.chroma_client = chromadb.PersistentClient(
            path=chroma_db_path,
            settings=Settings(
                anonymized_telemetry=False
            )
        )
        self.collection = self.chroma_client.get_or_create_collection(
            name="documents_search",
            metadata={"hnsw:space": "cosine"}
        )
        self.documents_metadata = {}  # –î–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É chunk_id –∏ document_id
        self.doc_id_to_text = {}  # –î–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        self._load_data_from_collection()

    def _load_data_from_collection(self):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏ ChromaDB
        :return:
        """
        try:
            count = self.collection.count()
            if count > 0:
                logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏ ({count} –∑–∞–ø–∏—Å–µ–π)...")

                # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
                all_data = self.collection.get(
                    include=["metadatas", "documents"]
                )

                # –ó–∞–ø–æ–ª–Ω—è–µ–º documents_metadata
                for i, (chunk_id, metadata, document_text) in enumerate(zip(
                        all_data["ids"],
                        all_data["metadatas"],
                        all_data["documents"]
                )):
                    doc_id = metadata["document_id"]
                    if i%1000 == 0:
                        logger.info(f"\r–ì–µ–Ω–µ—Ä–∞—Ü–∏—è {i}")

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                    self.documents_metadata[chunk_id] = {
                        "document_id": doc_id,
                        "chunk_index": metadata["chunk_index"],
                        "chunk_text": document_text,
                        "total_chunks": metadata["total_chunks"]
                    }

                    # –î–ª—è doc_id_to_text –Ω–∞–º –Ω—É–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
                    # –ü–æ—Å–∫–æ–ª—å–∫—É –º—ã —Ö—Ä–∞–Ω–∏–º —Ç–æ–ª—å–∫–æ chunks, —Å–æ–±–µ—Ä–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–∑ –≤—Å–µ—Ö chunks
                    if doc_id not in self.doc_id_to_text:
                        # –ù–∞–π–¥–µ–º –≤—Å–µ chunks –¥–ª—è —ç—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                        doc_chunks = self.collection.get(
                            where={"document_id": doc_id},
                            include=["documents", "metadatas"]
                        )

                        # –°–æ–±–µ—Ä–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–≤ chunks –ø–æ –∏–Ω–¥–µ–∫—Å—É
                        chunks_with_index = []
                        for chunk_meta, chunk_text in zip(doc_chunks["metadatas"], doc_chunks["documents"]):
                            chunks_with_index.append((chunk_meta["chunk_index"], chunk_text))

                        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∏–Ω–¥–µ–∫—Å—É –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º
                        chunks_with_index.sort(key=lambda x: x[0])
                        full_text = " ".join([chunk[1] for chunk in chunks_with_index])
                        self.doc_id_to_text[doc_id] = full_text

                logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.documents_metadata)} chunks –∏ {len(self.doc_id_to_text)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            else:
                logger.info("–ö–æ–ª–ª–µ–∫—Ü–∏—è –ø—É—Å—Ç–∞—è, –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –ø—É—Å—Ç—ã–µ —Å–ª–æ–≤–∞—Ä–∏")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {e}")
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—É—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞—Ä—è–º–∏ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            self.documents_metadata = {}
            self.doc_id_to_text = {}


    def preprocess_text(self, text: str) -> str:
        """
        –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ —Å—Ç—Ä–æ–∫
        :param text: –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        :return: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        text = re.sub(r'\s+', ' ', text.strip())
        return text

    def split_into_sentences(self, text: str) -> List[str]:
        """
        –†–∞–∑–±–∏—Ç–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        :param text: —Ç–µ–∫—Å—Ç
        :return: –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        """
        text = self.preprocess_text(text)
        sentences = [sentence.text for sentence in sentenize(text)]
        return sentences

    def create_chunks(self, sentences: List[str], chunk_size: int = 3, overlap: int = 1) -> List[str]:
        """
        –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ chunks (–æ—Ç—Ä–µ–∑–∫–∏).
        :param sentences: —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        :param chunk_size: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –æ–¥–Ω–æ–º chunk
        :param overlap:
        :return: —Å–ø–∏—Å–æ–∫ chunks
        """
        chunks = []
        for i in range(0, len(sentences), chunk_size - overlap):
            chunk = ' '.join(sentences[i:i + chunk_size])
            chunks.append(chunk)
            if i + chunk_size >= len(sentences):
                break
        return chunks

    def add_documents(self, documents: List[Dict[str, Any]],
                      chunk_size: int = 3,
                      overlap: int = 1,
                      batch_size: int = 1000):
        """
        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É.
        :param documents: —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∫–ª—é—á–∞–º–∏ 'id' –∏ 'text'
        :param chunk_size: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –æ–¥–Ω–æ–º chunk
        :param overlap:
        :param batch_size:
        """
        try:
            if not self.collection:
                self.collection = self.chroma_client.get_or_create_collection(
                    name="documents_search",
                    metadata={"hnsw:space": "cosine"}
                )

            all_chunks = []
            all_metadatas = []
            all_ids = []

            logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")

            for doc in tqdm(documents, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"):
                doc_id = doc['id']
                text = doc['text']
                self.doc_id_to_text[doc_id] = text

                # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                sentences = self.split_into_sentences(text)

                # –°–æ–∑–¥–∞–µ–º chunks –∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
                chunks = self.create_chunks(sentences, chunk_size)

                for chunk_idx, chunk_text in enumerate(chunks):
                    chunk_id = f"{doc_id}_chunk_{chunk_idx}"

                    all_chunks.append(chunk_text)
                    all_metadatas.append({"document_id": doc_id, "chunk_index": chunk_idx, "total_chunks": len(chunks)})
                    all_ids.append(chunk_id)

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
                    self.documents_metadata[chunk_id] = {
                        "document_id": doc_id,
                        "chunk_index": chunk_idx,
                        "chunk_text": chunk_text,
                        "total_chunks": len(chunks)
                    }
            logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(all_chunks)} chunks. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")

            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –±–∞—Ç—á–∞–º–∏
            embeddings = []
            for i in tqdm(range(0, len(all_chunks), batch_size), desc="–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"):
                batch_chunks = all_chunks[i:i + batch_size]
                batch_embeddings = self.model.encode(batch_chunks,
                                                 show_progress_bar=False,
                                                 batch_size=32)
                embeddings.extend(batch_embeddings.tolist())

            logger.info("–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É...")

            for i in tqdm(range(0, len(all_chunks), batch_size), desc="–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ ChromaDB"):
                end_idx = min(i + batch_size, len(all_chunks))

                self.collection.add(
                    embeddings=embeddings[i:end_idx],
                    documents=all_chunks[i:end_idx],
                    metadatas=all_metadatas[i:end_idx],
                    ids=all_ids[i:end_idx]
                )

            logger.info(f"–£—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —Ä–∞–∑–±–∏—Ç—ã—Ö –Ω–∞ {len(all_chunks)} chunks")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            raise

        print(f"–î–æ–±–∞–≤–ª–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —Ä–∞–∑–±–∏—Ç—ã—Ö –Ω–∞ {len(all_chunks)} chunks")

    def search(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """
        –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É
        :param query: –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (–∫–æ—Ä–æ—Ç–∫–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ)
        :param top_k: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        :return: —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –æ—Ü–µ–Ω–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        """
        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = self.model.encode([query])[0].tolist()

        self.collection = self.chroma_client.get_or_create_collection(
            name="documents_search",
            metadata={"hnsw:space": "cosine"}
        )

        # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ chunks
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k * 5,  # –ò—â–µ–º –±–æ–ª—å—à–µ chunks –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
            include=["metadatas", "documents", "distances"]
        )

        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ document_id
        doc_scores = {}
        doc_chunks = {}

        for i, (metadata, chunk_text, distance) in enumerate(zip(
                results['metadatas'][0],
                results['documents'][0],
                results['distances'][0]
        )):
            doc_id = metadata['document_id']
            similarity_score = 1 - distance  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –≤ —Å—Ö–æ–∂–µ—Å—Ç—å

            if doc_id not in doc_scores:
                doc_scores[doc_id] = []
                doc_chunks[doc_id] = []

            doc_scores[doc_id].append(similarity_score)
            doc_chunks[doc_id].append({
                "text": chunk_text,
                "score": similarity_score,
                "chunk_index": metadata['chunk_index']
            })

        # –í—ã—á–∏—Å–ª—è–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ score –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        aggregated_results = []
        for doc_id, scores in doc_scores.items():
            # –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏:
            # max_score = np.max(scores)          # —Å–∞–º—ã–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π chunk
            avg_score = np.mean(scores)  # —Å—Ä–µ–¥–Ω—è—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
            # sum_score = np.sum(scores)          # —Å—É–º–º–∞—Ä–Ω–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å

            aggregated_results.append({
                "document_id": doc_id,
                "score": avg_score,
                "original_text": self.doc_id_to_text[doc_id],
                "matching_chunks": doc_chunks[doc_id],
                "chunks_count": len(scores)
            })

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        aggregated_results.sort(key=lambda x: x['score'], reverse=True)

        return aggregated_results[:top_k]

    def print_results(self, results: List[Dict[str, Any]]):
        """–ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞"""
        for i, result in enumerate(results, 1):
            print(f"\n{i}. –î–æ–∫—É–º–µ–Ω—Ç ID: {result['document_id']}")
            print(f"   –û–±—â–∏–π score: {result['score']:.4f}")
            print(f"   –ù–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö chunks: {result['chunks_count']}")
            print(f"   –°–∞–º—ã–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π chunk:")
            best_chunk = max(result['matching_chunks'], key=lambda x: x['score'])
            print(f"   '{best_chunk['text']}' (score: {best_chunk['score']:.4f})")
            print("-" * 80)


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞
    search_engine = LongTextSearchEngine()

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤)
    documents = [
        {
            "id": "doc_1",
            "text": """
            –ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–∏–∑–∏—Ä—É–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—É—é –º–µ–¥–∏—Ü–∏–Ω—É. 
            –ù–æ–≤—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø–æ–∑–≤–æ–ª—è—é—Ç –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è 
            –Ω–∞ —Ä–∞–Ω–Ω–∏—Ö —Å—Ç–∞–¥–∏—è—Ö —Å –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ 
            –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç outstanding —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∞–Ω–∞–ª–∏–∑–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. 
            –í—Ä–∞—á–∏ –≤—Å–µ–≥–æ –º–∏—Ä–∞ –Ω–∞—á–∏–Ω–∞—é—Ç –≤–Ω–µ–¥—Ä—è—Ç—å AI-—Å–∏—Å—Ç–µ–º—ã –≤ —Å–≤–æ—é –ø—Ä–∞–∫—Ç–∏–∫—É.
            """
        },
        {
            "id": "doc_2",
            "text": """
            –ö—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã –∏ –±–ª–æ–∫—á–µ–π–Ω —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –ø—Ä–æ–¥–æ–ª–∂–∞—é—Ç —Ä–∞–∑–≤–∏–≤–∞—Ç—å—Å—è –±—ã—Å—Ç—Ä—ã–º–∏ —Ç–µ–º–ø–∞–º–∏.
            Bitcoin –∏ Ethereum –æ—Å—Ç–∞—é—Ç—Å—è –ª–∏–¥–µ—Ä–∞–º–∏ —Ä—ã–Ω–∫–∞, –Ω–æ –ø–æ—è–≤–ª—è—é—Ç—Å—è –∏ –Ω–æ–≤—ã–µ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ–µ–∫—Ç—ã.
            –î–µ—Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∏–Ω–∞–Ω—Å—ã (DeFi) –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–π –±–∞–Ω–∫–æ–≤—Å–∫–æ–π —Å–∏—Å—Ç–µ–º–µ.
            –ú–Ω–æ–≥–∏–µ –∏–Ω–≤–µ—Å—Ç–æ—Ä—ã –≤–∏–¥—è—Ç –≤ —Ü–∏—Ñ—Ä–æ–≤—ã—Ö –∞–∫—Ç–∏–≤–∞—Ö –∑–∞—â–∏—Ç—É –æ—Ç –∏–Ω—Ñ–ª—è—Ü–∏–∏.
            """
        },
        {
            "id": "doc_3",
            "text": """
            –ò–∑–º–µ–Ω–µ–Ω–∏–µ –∫–ª–∏–º–∞—Ç–∞ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –æ–¥–Ω–æ–π –∏–∑ —Å–∞–º—ã—Ö urgent –ø—Ä–æ–±–ª–µ–º —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ—Å—Ç–∏. 
            –ì–ª–æ–±–∞–ª—å–Ω–æ–µ –ø–æ—Ç–µ–ø–ª–µ–Ω–∏–µ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–º –ø–æ–≥–æ–¥–Ω—ã–º —É—Å–ª–æ–≤–∏—è–º –ø–æ –≤—Å–µ–º—É –º–∏—Ä—É. 
            –£—á–µ–Ω—ã–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞—é—Ç –æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –≤—ã–±—Ä–æ—Å–æ–≤ –ø–∞—Ä–Ω–∏–∫–æ–≤—ã—Ö –≥–∞–∑–æ–≤. 
            –í–æ–∑–æ–±–Ω–æ–≤–ª—è–µ–º—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —ç–Ω–µ—Ä–≥–∏–∏ - –∫–ª—é—á –∫ sustainable –±—É–¥—É—â–µ–º—É.
            """
        }
    ]

    # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ —Å–∏—Å—Ç–µ–º—É
    search_engine.add_documents(documents, chunk_size=2)

    # –ü—Ä–∏–º–µ—Ä—ã –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (–∫–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
    queries = [
        "–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞",
        "–±–∏—Ç–∫–æ–∏–Ω –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏",
        "–≥–ª–æ–±–∞–ª—å–Ω–æ–µ –ø–æ—Ç–µ–ø–ª–µ–Ω–∏–µ"
    ]

    # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
    for query in queries:
        print(f"\nüîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{query}'")
        print("=" * 80)

        results = search_engine.search(query, top_k=2)
        search_engine.print_results(results)